\documentclass[12pt]{article}
\usepackage[italian]{babel}
\usepackage{natbib}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{float}
\usepackage{hyperref}
\usepackage{subcaption}
\setmarginsrb{3 cm}{2.5 cm}{3 cm}{2.5 cm}{1 cm}{1.5 cm}{1 cm}{1.5 cm}

\title{HAR Bayesian Network}								% Title
\author{Artifoni Mattia \\ Brena Luca \\ Bottoni Federico}								% Author
\date{Giugno 2019}											% Date

\makeatletter
\let\thetitle\@title
\let\theauthor\@author
\let\thedate\@date
\makeatother

\pagestyle{fancy}
\fancyhf{}
\rhead{Artifoni M. Brena L. Bottoni F.}
\lhead{\thetitle}
\cfoot{\thepage}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
	\centering
    \vspace*{0.5 cm}
    \includegraphics[scale = 0.75]{images/LogoBicocca.pdf}\\[1.0 cm]	% University Logo
    \textsc{\LARGE Università degli studi di}\\[0.2 cm]
    \textsc{\LARGE Milano-Bicocca}\\[2.0 cm]	% University Name
	\textsc{\Large F1801Q145}\\[0.5 cm]				% Course Code
	\textsc{\large Modelli probabilistici per le decisioni}\\[0.5 cm]				% Course Name
	\rule{\linewidth}{0.2 mm} \\[0.4 cm]
	{ \huge \bfseries \thetitle}\\
	\rule{\linewidth}{0.2 mm} \\[1.5 cm]

	\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \large
			\emph{Studenti:}\\
			\theauthor
			\end{flushleft}
			\end{minipage}~
			\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
			\emph{Matricole:} \\
			807466 \\ 808216 \\ 806944
		\end{flushright}
	\end{minipage}\\[2 cm]

	{\large \thedate}\\[2 cm]

	\vfill

\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Introduzione}
Il progetto ha l'obiettivo di creare un modello di Rete Bayesiana capace di predirre il tipo di azione che sta effettuando un ipotetico individuo che indossa il "HAR wearable devices setup", una particolare sistema indossabile composto da 4 accelerometri che permette di analizzare i vettori accelerazione dei sensori in questione. Viene fornito dal progetto di riferimento\cite{HAR} un dataset contenente dati sufficienti per effettuare training e testing del modello

\subsection{Dominio di riferimento}
La semantica dei dati utilizzati è definita nel paper\cite{Paper} del progetto di provenienza. La singola entry del dataset rappresenta uno snapshot acquisito dai sensori e consiste in:
\begin{itemize}
	\item user: username dell'individuo in oggetto (string).
	\item gender: genere del soggetto (string).
	\item age: età dell'individuo (int).
	\item how\_tall\_in\_meters: altezza del soggetto espressa in metri (int).
	\item weight: peso espresso in kilogrammi (int).
	\item body\_mass\_index: indice di massa corporea. Si ottiene dividendo il peso per il quadrato dell'altezza (float).
	\item xi: intero che esprime la componente x del vettore accelerazione nel sensore i-esimo (int).
	\item yi: intero che esprime la componente y del vettore accelerazione nel sensore i-esimo (int).
	\item zi: intero che esprime la componente z del vettore accelerazione nel sensore i-esimo (int).
	\item class: è la variabile target della previsione e indica l'azione eseguita dal soggetto al momento della rilevazione dei dati. Può assumere il valore di "walking", "standing", "standingup", "sitting" e "sittingdown" (string).
\end{itemize}


\subsection{Ipotesi e assunzioni}
Durante lo studio del caso sono state discrimate le features utili al training della rete (i vettori dei sensori) da quelle assunte come superflue (user, gender, age, weight, body\_mass\_index) le quali potrebbero essere utilizzate per specializzarla ulteriormente.\newline
La scelta riguardo all'attributo \emph{how\_tall\_in\_meters} non è stata particolarmente immediata dato che il training set considera un range di 13cm (1.58m - 1.71m) che distribuiti in un corpo umano non crea l'informazione necessaria per poter affermare che tutti i sensori si trovano 13cm più o meno vicini al terreno. La rete dovrebbe essere comunque in grado di predirre le azioni di un bambino, il quale ha altezza decisamente inferiore rispetto a quella precedentemente descritta, ciò nonostante assumiamo che l'utente abbia un'altezza nel range descritto dato che in alcuni test affrontati, la complessità della rete era tale da scatenare MemoryError nella rappresentazione dei dati.

\section{Scelte di design}
\subsection{Analisi statistica e qualitativa}
\begin{figure}[H]
	\centering
	{\includegraphics[width=1\textwidth]{images/dataset.JPG}}
	\caption{Dataset dopo lo shuffle}
	\label{fig:dataset}
\end{figure}
Il dataset si presenta come in figura \ref{fig:dataset} dopo una prima fase di pulizia, in cui sono stati individuati alcuni caratteri non necessari tra i campi e timestamp inaspettati tra le entry della tabella, ed una seconda di shuffle, nella quale i record sono stati randomizzati. \newline
E' stata effettuata inoltre una fase di analisi statistico-descrittiva considerando le features che assumono valori in range indefiniti per cercare di individuare qualche distribuzione particolare o comportamento anomalo.


\begin{table}[h]
	\caption{Analisi descrittiva del dataset}\label{tab:analytics}
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		Campo & Min & Max & Media & Moda & DevStd \\
		\hline
		x1 & -306 & 509 & -6.649327127 & -1 & 11.61623803 \\
		y1 & -271 & 533 & 88.29366732 & 95 & 23.89582898 \\
		z1 & -603 & 411 & -93.16461092 & -98 & 39.40942342 \\
		x2 & -494 & 473 & -87.82750418 & -492 & 169.4351938 \\
		y2 & -517 & 295 & -52.06504742 & -516 & 205.1597632 \\
		z2 & -617 & 122 & -175.0552004 & -616 & 192.8166147 \\
		x3 & -499 & 507 & 17.42351464 & 38 & 52.63538753 \\
		y3 & -506 & 517 & 104.5171675 & 108 & 54.15584251 \\
		z3 & -613 & 410 & -93.88172647 & -102 & 45.38964613 \\
		x4 & -702 & -13 & -167.6414483 & -164 & 38.31134199 \\
		y4 & -526 & 86 & -92.62517131 & -94 & 19.96861022 \\
		z4 & -537 & -43 & 88.29366732 & -162 & 13.22102006 \\
		
		\hline
	\end{tabular}
\end{table}

Dalla tabella \ref{tab:analytics} si può notare che i range di variabilità degli attributi non seguono comportamenti particolari, tanto meno le distribuzioni che in alcuni casi sono caratterizzati da deviazione standard particolarmente bassa (come il caso di \emph{x1}) mentre in altri casi molto alta (come \emph{y2}).


\begin{figure}[h]
	
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=0.9\linewidth, height=5cm]{images/x1notnormalize.PNG} 
		\caption{Distribuzione dei valori di x1 non normalizzati.}
		\label{fig:x1nonNormalize}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=0.9\linewidth, height=5cm]{images/x1normalize.PNG}
		\caption{Distribuzione dei valori di x1 normalizzati.}
		\label{fig:subim2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=0.9\linewidth, height=5cm]{images/x1discretize.PNG}
		\caption{Distribuzione dei valori di x1 normalizzati e discretizzati.}
		\label{fig:subim2}
	\end{subfigure}
	\caption{Normalizzazione e discretizzazione della feature x1.}
	\label{fig:x1}
\end{figure}

\subsection{Normalizzazione}
Nella fase embrionale della progettazione la normalizzazione era stata ignorata. Infatti la predizione lavorava su features che avevano range piuttosto incosistenti al variare del sensore e della componente del vettore accelerazione considerata. Questo si può notare dalle colonne \textit{Min} e \textit{Max} in tabella \ref{tab:analytics}, che si riferisce ai dati grezzi parzialmente ripuliti. Quindi si è pensato di effettuare una normalizzazione per ogni feature, in modo da compattare i valori tra -1 e 1 andando anche a eliminare gli outliers. \par
Di seguito, nelle figure \ref{fig:x1} e \ref{fig:z4}, sono presentati alcuni grafici che illustrano la distribuzione dei valori di alcune features prima e dopo la normalizzazione. A seguito dell'operazione di normalizzazione il dataset appare come riportato in figura \ref{fig:normalized}.

\begin{figure}[H]
	\includegraphics[width=1\textwidth]{images/datasetNormalizzato.PNG}
	\caption{Il dataset normalizzato.}
	\label{fig:normalized}
\end{figure}


\begin{figure}[h]
	
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=0.9\linewidth, height=5cm]{images/z4notnormalize.PNG} 
		\caption{Distribuzione dei valori di z4 non normalizzati.}
		\label{fig:x1nonNormalize}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=0.9\linewidth, height=5cm]{images/z4normalize.PNG}
		\caption{Distribuzione dei valori di z4 normalizzati.}
		\label{fig:subim2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=0.9\linewidth, height=5cm]{images/z4discretize.PNG}
		\caption{Distribuzione dei valori di z4 normalizzati e discretizzati.}
		\label{fig:subim2}
	\end{subfigure}
	\caption{Normalizzazione e discretizzazione della feature x1.}
	\label{fig:z4}
\end{figure}

\subsection{Discretizzazione}
I valori delle features nel dataset originale sono continui. Per lavorare con le reti Bayesiane è stato necessario ricorrere alla discretizzazione dei dati.\par
Inizialmente i dati sono stati suddivisi in partizioni a range identici senza tenere conto di come questi fossero distribuiti. Successivamente abbiamo realizzato come questa procedura fosse imprecisa dopo aver osservato il modo in cui le distribuzioni di probabilità si sbilanciavano prevalentemente verso una sola classe della variabile. \newline
La discretizzazione definitiva è stata applicata sul dataset normalizzato. Per discretizzare i dati sono stati scelti 5 intervalli o bins. La funzione scelta (KBinsDiscretizer della libreria sklearn \cite{scikit-learn}) adatta automaticamente il numero dei bins in modo che la distribuzione dei dati in essi sia omogenea. Uno sapshot dei dati discretizzati è raffigurato nell'immagine \ref{fig:discretized}

\begin{figure}[h]
	\includegraphics[scale = 0.5]{images/datasetDiscretizzato.PNG}
	\caption{Il dataset discretizzato.}
	\label{fig:discretized}
\end{figure}


\section{I modelli di rete}
\subsection{pgmpy}
Il software inizialmente scelto è pgmpy\cite{pgmpy} di Python, una libreria che permette di modellare le dipendenze in modo agile e stimare le CPT delle variabili sfruttando dei metodi che accettano il dataset ed infine effettuare inferenze dichiarando la variabile di query e le evidenze. \linebreak
Utilizzando la libreria ci siamo resi conto di come sia performante utilizzando modelli semplici e correlati da pochi record, tuttavia appena è avvenuta l'esecuzione della stima delle CPT nel primo modello completo ideato abbiamo riscontrato le prime difficoltà.

\subsubsection{Modello correlato}
L'idea che sta alla base di questo primo modello consiste nello stimare le dipendenze tramite lo strumento statistico \emph{indice di correlazione di Paerson}, assumendo che se due feautres hanno distribuzioni simili (e quindi alta correlazione diretta o inversa) allora vi è una dipendenza tra le due. Sono state così selezionate le relazioni scartando le simmetrie e stimate le CPT tramite il metodo \emph{MaximumLikelihoodEstimator} della libreria basandosi sui sample passati come parametro. \newline

\begin{figure}[H]
	\centering
	{\includegraphics[width=1\textwidth]{images/corr05.JPG}}
	\caption{Indice di correlazione di Paerson calcolato su tutte le le combinazioni di componenti}
	\label{fig:corr}
\end{figure}

Successivamente abbiamo stimato la precisione del modello calcolando il rapporto tra le inferenze corrette su quelle totali. Il metodo utilizzato, \emph{query}, si occupa di effettuare la singola previsione. Abbiamo notato che a parità di modello, raddoppiando il numero delle classi per ogni variabile, il tempo impiegato dalla funzione aumenta vertiginosamente, perciò abbiamo stimato la precisione del modello più complesso possibile ma con tempi di esecuzione nella norma. \newline
Il modello consiste in cinque classi per ogni variabile e le seguenti dipendenze tra le variabili:\newline \emph{[('x1', 'class'),('x3', 'class'),('y4', 'class'), ('z1', 'class'), ('z2', 'class'), ('z3', 'class'), ('z4', 'class'), ('y1', 'z1'), ('x2', 'y2'), ('x2', 'z2'), ('y2', 'z2'), ('y3', 'z3'), ('x4', 'z1'),  ('x4', 'y4')]}.\newline
La precisione stimata è del 54\%, un valore troppo basso per giustificare la computazione così lunga e dispendiosa di risorse. Abbiamo deciso così di tentare con un modello differente.

\subsubsection{Generazione del modello}
Dato l'insuccesso del modello correlato abbiamo cercato di generare la miglior configurazione di dipendenze secondo la libreria. Dopo il lancio della funzione che si occupa di chiamare l'API per la generazione abbiamo atteso circa 8 ore e successivamente interrotto l'esecuzione. Forse per la mole di dati, forse per la discretizzazione del dataset o forse per la natura stessa dell'algoritmo, non vi è stato alcun risultato.

\subsection{pomegranate}
Alla luce dei test effettuati abbiamo deciso di utilizzare un'altra libreria: Pomegranate \cite{pomegranate}, secondo gli utenti di alcuni forum, dovrebbe essere performante anche nei casi in cui \emph{pgmpy} non lo è. La libreria è nota inoltre per avere API molto simili a quelle di scikit-learn \cite{scikit-learn} (uno tra i software leader nel campo del Machine Learning) 

\subsubsection{Modello generato}
\clearpage
\section{Risultati e conclusioni}

\newpage
\bibliographystyle{plain}
\bibliography{biblist}

\end{document}
